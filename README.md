# Лабораторная работа №4: ETL-процесс в Apache Spark

## Цель работы

**Изучить принципы и реализацию ETL-процессов (Extract, Transform, Load) в современных информационных системах и научиться выполнять ETL-задачи с использованием Apache Spark (PySpark).**  

---

## Теоретическая часть

### ETL и ELT: сравнение подходов

- **ETL (Extract, Transform, Load)** – данные извлекаются, преобразуются и только потом загружаются в хранилище.  
- **ELT (Extract, Load, Transform)** – данные сначала загружаются в хранилище, а потом там же трансформируются.  

**Extract (извлечение):**
- Полное извлечение.
- Инкрементальное с уведомлениями (триггеры, CDC).
- Инкрементальное без уведомлений (по временным меткам, хешам).

**Transform (преобразование):**
- Очистка данных: удаление дублей, обработка `NULL`, исправление форматов.
- Приведение типов.
- Агрегации, вычисления дополнительных атрибутов.
- Подготовка к структуре хранилища (факты, измерения).

**Load (загрузка):**
- Загрузка преобразованных данных в DWH или Data Lake.
- Полная или инкрементальная загрузка.
- Контроль качества загрузки.

**Инструменты:**
- ETL-платформы: Informatica, SSIS, Talend, Pentaho, NiFi.
- Облачные решения: AWS Glue, Google Dataflow, Azure Data Factory.
- Big Data-платформы: **Apache Spark** (с PySpark API).

**Apache Spark (PySpark):**
- Распределённая обработка больших данных.
- API DataFrame и SQL.
- Коннекторы к CSV, JSON, JDBC, HDFS, S3.
- Подходит для ETL-конвейеров от извлечения до загрузки.
